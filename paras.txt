b'HomepageHomepageSign inGet startedData ScienceMachine LearningProgrammingVisualizationAIJournalismPicksContributeA One-Stop Shop for Principal Component AnalysisMatt BremsBlockedUnblockFollowFollowingApr 17, 2017At the beginning of the textbook I used for my graduate stat theory class, the authors (George Casella and Roger Berger) explained in the preface why they chose to write a textbook:\xe2\x80\x9cWhen someone discovers that you are writing a textbook, one or both of two questions will be asked. The first is \xe2\x80\x9cWhy are you writing a book?\xe2\x80\x9d and the second is \xe2\x80\x9cHow is your book different from what\xe2\x80\x99s out there?\xe2\x80\x9d The first question is fairly easy to answer. You are writing a book because you are not entirely satisfied with the available texts.\xe2\x80\x9dI apply the authors\xe2\x80\x99 logic here. Principal component analysis (PCA) is an important technique to understand in the fields of statistics and data science\xe2\x80\xa6 but when putting a lesson together for my General Assembly students, I found that the resources online were too technical, didn\xe2\x80\x99t fully address our needs, and/or provided conflicting information. It\xe2\x80\x99s safe to say that I\xe2\x80\x99m not \xe2\x80\x9centirely satisfied with the available texts\xe2\x80\x9d here.As a result, I wanted to put together the \xe2\x80\x9cWhat,\xe2\x80\x9d \xe2\x80\x9cWhen,\xe2\x80\x9d \xe2\x80\x9cHow,\xe2\x80\x9d and \xe2\x80\x9cWhy\xe2\x80\x9d of PCA as well as links to some of the resources that can help to further explain this topic. Specifically, I want to present the rationale for this method, the math under the hood, some best practices, and potential drawbacks to the method.While I want to make PCA as accessible as possible, the algorithm we\xe2\x80\x99ll cover is pretty technical. Being familiar with some or all of the following will make this article and PCA as a method easier to understand: matrix operations/linear algebra (matrix multiplication, matrix transposition, matrix inverses, matrix decomposition, eigenvectors/eigenvalues) and statistics/machine learning (standardization, variance, covariance, independence, linear regression, feature selection). I\xe2\x80\x99ve embedded links to illustrations of these topics throughout the article, but hopefully these will serve as a reminder rather than required reading to get through the article.What is\xc2\xa0PCA?Let\xe2\x80\x99s say that you want to predict what the gross domestic product (GDP) of the United States will be for 2017. You have lots of information available: the U.S. GDP for the first quarter of 2017, the U.S. GDP for the entirety of 2016, 2015, and so on. You have any publicly-available economic indicator, like the unemployment rate, inflation rate, and so on. You have U.S. Census data from 2010 estimating how many Americans work in each industry and American Community Survey data updating those estimates in between each census. You know how many members of the House and Senate belong to each political party. You could gather stock price data, the number of IPOs occurring in a year, and how many CEOs seem to be mounting a bid for public office. Despite being an overwhelming number of variables to consider, this just scratches the surface.TL;DR\xe2\x80\x8a\xe2\x80\x94\xe2\x80\x8ayou have a lot of variables to consider.If you\xe2\x80\x99ve worked with a lot of variables before, you know this can present problems. Do you understand the relationships between each variable? Do you have so many variables that you are in danger of overfitting your model to your data or that you might be violating assumptions of whichever modeling tactic you\xe2\x80\x99re using?You might ask the question, \xe2\x80\x9cHow do I take all of the variables I\xe2\x80\x99ve collected and focus on only a few of them?\xe2\x80\x9d In technical terms, you want to \xe2\x80\x9creduce the dimension of your feature space.\xe2\x80\x9d By reducing the dimension of your feature space, you have fewer relationships between variables to consider and you are less likely to overfit your model. (Note: This doesn\xe2\x80\x99t immediately mean that overfitting, etc. are no longer concerns\xe2\x80\x8a\xe2\x80\x94\xe2\x80\x8abut we\xe2\x80\x99re moving in the right direction!)Somewhat unsurprisingly, reducing the dimension of the feature space is called \xe2\x80\x9cdimensionality reduction.\xe2\x80\x9d There are many ways to achieve dimensionality reduction, but most of these techniques fall into one of two classes:Feature EliminationFeature ExtractionFeature elimination is what it sounds like: we reduce the feature space by eliminating features. In the GDP example above, instead of considering every single variable, we might drop all variables except the three we think will best predict what the U.S.\xe2\x80\x99s gross domestic product will look like. Advantages of feature elimination methods include simplicity and maintaining interpretability of your variables.As a disadvantage, though, you gain no information from those variables you\xe2\x80\x99ve dropped. If we only use last year\xe2\x80\x99s GDP, the proportion of the population in manufacturing jobs per the most recent American Community Survey numbers, and unemployment rate to predict this year\xe2\x80\x99s GDP, we\xe2\x80\x99re missing out on whatever the dropped variables could contribute to our model. By eliminating features, we\xe2\x80\x99ve also entirely eliminated any benefits those dropped variables would bring.Feature extraction, however, doesn\xe2\x80\x99t run into this problem. Say we have ten independent variables. In feature extraction, we create ten \xe2\x80\x9cnew\xe2\x80\x9d independent variables, where each \xe2\x80\x9cnew\xe2\x80\x9d independent variable is a combination of each of the ten \xe2\x80\x9cold\xe2\x80\x9d independent variables. However, we create these new independent variables in a specific way and order these new variables by how well they predict our dependent variable.You might say, \xe2\x80\x9cWhere does the dimensionality reduction come into play?\xe2\x80\x9d Well, we keep as many of the new independent variables as we want, but we drop the \xe2\x80\x9cleast important ones.\xe2\x80\x9d Because we ordered the new variables by how well they predict our dependent variable, we know which variable is the most important and least important. But\xe2\x80\x8a\xe2\x80\x94\xe2\x80\x8aand here\xe2\x80\x99s the kicker\xe2\x80\x8a\xe2\x80\x94\xe2\x80\x8abecause these new independent variables are combinations of our old ones, we\xe2\x80\x99re still keeping the most valuable parts of our old variables, even when we drop one or more of these \xe2\x80\x9cnew\xe2\x80\x9d variables!Principal component analysis is a technique for feature extraction\xe2\x80\x8a\xe2\x80\x94\xe2\x80\x8aso it combines our input variables in a specific way, then we can drop the \xe2\x80\x9cleast important\xe2\x80\x9d variables while still retaining the most valuable parts of all of the variables! As an added benefit, each of the \xe2\x80\x9cnew\xe2\x80\x9d variables after PCA are all independent of one another. This is a benefit because the assumptions of a linear model require our independent variables to be independent of one another. If we decide to fit a linear regression model with these \xe2\x80\x9cnew\xe2\x80\x9d variables (see \xe2\x80\x9cprincipal component regression\xe2\x80\x9d below), this assumption will necessarily be satisfied.When should I use\xc2\xa0PCA?Do you want to reduce the number of variables, but aren\xe2\x80\x99t able to identify variables to completely remove from consideration?Do you want to ensure your variables are independent of one another?Are you comfortable making your independent variables less interpretable?If you answered \xe2\x80\x9cyes\xe2\x80\x9d to all three questions, then PCA is a good method to use. If you answered \xe2\x80\x9cno\xe2\x80\x9d to question 3, you should not use PCA.How does PCA\xc2\xa0work?The section after this discusses why PCA works, but providing a brief summary before jumping into the algorithm may be helpful for context:We are going to calculate a matrix that summarizes how our variables all relate to one another.We\xe2\x80\x99ll then break this matrix down into two separate components: direction and magnitude. We can then understand the \xe2\x80\x9cdirections\xe2\x80\x9d of our data and its \xe2\x80\x9cmagnitude\xe2\x80\x9d (or how \xe2\x80\x9cimportant\xe2\x80\x9d each direction is). The screenshot below, from the setosa.io applet, displays the two main directions in this data: the \xe2\x80\x9cred direction\xe2\x80\x9d and the \xe2\x80\x9cgreen direction.\xe2\x80\x9d In this case, the \xe2\x80\x9cred direction\xe2\x80\x9d is the more important one. We\xe2\x80\x99ll get into why this is the case later, but given how the dots are arranged, can you see why the \xe2\x80\x9cred direction\xe2\x80\x9d looks more important than the \xe2\x80\x9cgreen direction?\xe2\x80\x9d (Hint: What would fitting a line of best fit to this data look like?)Our original data in the xy-plane. (Source.)We will transform our original data to align with these important directions (which are combinations of our original variables). The screenshot below (again from setosa.io) is the same exact data as above, but transformed so that the x- and y-axes are now the \xe2\x80\x9cred direction\xe2\x80\x9d and \xe2\x80\x9cgreen direction.\xe2\x80\x9d What would the line of best fit look like here?Our original data transformed by PCA. (Source.)While the visual example here is two-dimensional (and thus we have two \xe2\x80\x9cdirections\xe2\x80\x9d), think about a case where our data has more dimensions. By identifying which \xe2\x80\x9cdirections\xe2\x80\x9d are most \xe2\x80\x9cimportant,\xe2\x80\x9d we can compress or project our data into a smaller space by dropping the \xe2\x80\x9cdirections\xe2\x80\x9d that are the \xe2\x80\x9cleast important.\xe2\x80\x9d By projecting our data into a smaller space, we\xe2\x80\x99re reducing the dimensionality of our feature space\xe2\x80\xa6 but because we\xe2\x80\x99ve transformed our data in these different \xe2\x80\x9cdirections,\xe2\x80\x9d we\xe2\x80\x99ve made sure to keep all original variables in our model!Here, I walk through an algorithm for conducting PCA. I try to avoid being too technical, but it\xe2\x80\x99s impossible to ignore the details here, so my goal is to walk through things as explicitly as possible. A deeper intuition of why the algorithm works is presented in the next section.Before starting, you should have tabular data organized with n rows and likely p+1 columns, where one column corresponds to your dependent variable (usually denoted Y) and p columns where each corresponds to an independent variable (the matrix of which is usually denoted X).If a Y variable exists and is part of your data, then separate your data into Y and X, as defined above\xe2\x80\x8a\xe2\x80\x94\xe2\x80\x8awe\xe2\x80\x99ll mostly be working with X. (Note: if there exists no column for Y, that\xe2\x80\x99s okay\xe2\x80\x8a\xe2\x80\x94\xe2\x80\x8askip to the next point!)Take the matrix of independent variables X and, for each column, subtract the mean of that column from each entry. (This ensures that each column has a mean of zero.)Decide whether or not to standardize. Given the columns of X, are features with higher variance more important than features with lower variance, or is the importance of features independent of the variance? (In this case, importance means how well that feature predicts Y.) If the importance of features is independent of the variance of the features, then divide each observation in a column by that column\xe2\x80\x99s standard deviation. (This, combined with step 2, standardizes each column of X to make sure each column has mean zero and standard deviation 1.) Call the centered (and possibly standardized) matrix Z.Take the matrix Z, transpose it, and multiply the transposed matrix by Z. (Writing this out mathematically, we would write this as Z\xe1\xb5\x80Z.) The resulting matrix is the covariance matrix of Z, up to a constant.(This is probably the toughest step to follow\xe2\x80\x8a\xe2\x80\x94\xe2\x80\x8astick with me here.) Calculate the eigenvectors and their corresponding eigenvalues of Z\xe1\xb5\x80Z. This is quite easily done in most computing packages\xe2\x80\x94 in fact, the eigendecomposition of Z\xe1\xb5\x80Z is where we decompose Z\xe1\xb5\x80Z into PDP\xe2\x81\xbb\xc2\xb9, where P is the matrix of eigenvectors and D is the diagonal matrix with eigenvalues on the diagonal and values of zero everywhere else. The eigenvalues on the diagonal of D will be associated with the corresponding column in P\xe2\x80\x8a\xe2\x80\x94\xe2\x80\x8athat is, the first element of D is \xce\xbb\xe2\x82\x81 and the corresponding eigenvector is the first column of P. This holds for all elements in D and their corresponding eigenvectors in P. We will always be able to calculate PDP\xe2\x81\xbb\xc2\xb9 in this fashion. (Bonus: for those interested, we can always calculate PDP\xe2\x81\xbb\xc2\xb9 in this fashion because Z\xe1\xb5\x80Z is a symmetric, positive semidefinite matrix.)Take the eigenvalues \xce\xbb\xe2\x82\x81, \xce\xbb\xe2\x82\x82,\xc2\xa0\xe2\x80\xa6, \xce\xbbp and sort them from largest to smallest. In doing so, sort the eigenvectors in P accordingly. (For example, if \xce\xbb\xe2\x82\x82 is the largest eigenvalue, then take the second column of P and place it in the first column position.) Depending on the computing package, this may be done automatically. Call this sorted matrix of eigenvectors P*. (The columns of P* should be the same as the columns of P, but perhaps in a different order.) Note that these eigenvectors are independent of one another.Calculate Z* = ZP*. This new matrix, Z*, is a centered/standardized version of X but now each observation is a combination of the original variables, where the weights are determined by the eigenvector. As a bonus, because our eigenvectors in P* are independent of one another, each column of Z* is also independent of one another!An example from setosa.io where we transform five data points using PCA. The left graph is our original data X; the right graph would be our transformed data\xc2\xa0Z*.Note two things in this graphic:The two charts show the exact same data, but the right graph reflects the original data transformed so that our axes are now the principal components.In both graphs, the principal components are perpendicular to one another. In fact, every principal component will ALWAYS be orthogonal (a.k.a. official math term for perpendicular) to every other principal component. (Don\xe2\x80\x99t believe me? Try to break the applet!)Because our principal components are orthogonal to one another, they are statistically independent of one another\xe2\x80\xa6 which is why our columns of Z* are independent of one another!8. Finally, we need to determine how many features to keep versus how many to drop. There are three common methods to determine this, discussed below and followed by an explicit example:Method 1: We arbitrarily select how many dimensions we want to keep. Perhaps I want to visually represent things in two dimensions, so I may only keep two features. This is use-case dependent and there isn\xe2\x80\x99t a hard-and-fast rule for how many features I should pick.Method 2: Calculate the proportion of variance explained (briefly explained below) for each feature, pick a threshold, and add features until you hit that threshold. (For example, if you want to explain 80% of the total variability possibly explained by your model, add features with the largest explained proportion of variance until your proportion of variance explained hits or exceeds 80%.)Method 3: This is closely related to Method 2. Calculate the proportion of variance explained for each feature, sort features by proportion of variance explained and plot the cumulative proportion of variance explained as you keep more features. (This plot is called a scree plot, shown below.) One can pick how many features to include by identifying the point where adding a new feature has a significant drop in variance explained relative to the previous feature, and choosing features up until that point. (I call this the \xe2\x80\x9cfind the elbow\xe2\x80\x9d method, as looking at the \xe2\x80\x9cbend\xe2\x80\x9d or \xe2\x80\x9celbow\xe2\x80\x9d in the scree plot determines where the biggest drop in proportion of variance explained occurs.)Because each eigenvalue is roughly the importance of its corresponding eigenvector, the proportion of variance explained is the sum of the eigenvalues of the features you kept divided by the sum of the eigenvalues of all features.Scree Plot for Genetic Data. (Source.)Consider this scree plot for genetic data. (Source: here.) The red line indicates the proportion of variance explained by each feature, which is calculated by taking that principal component\xe2\x80\x99s eigenvalue divided by the sum of all eigenvalues. The proportion of variance explained by including only principal component 1 is \xce\xbb\xe2\x82\x81/(\xce\xbb\xe2\x82\x81 + \xce\xbb\xe2\x82\x82 +\xc2\xa0\xe2\x80\xa6 + \xce\xbbp), which is about 23%. The proportion of variance explained by including only principal component 2 is \xce\xbb\xe2\x82\x82/(\xce\xbb\xe2\x82\x81 + \xce\xbb\xe2\x82\x82 +\xc2\xa0\xe2\x80\xa6 + \xce\xbbp), or about 19%.The proportion of variance explained by including both principal components 1 and 2 is (\xce\xbb\xe2\x82\x81 + \xce\xbb\xe2\x82\x82)/(\xce\xbb\xe2\x82\x81 + \xce\xbb\xe2\x82\x82 +\xc2\xa0\xe2\x80\xa6 + \xce\xbbp), which is about 42%. This is where the yellow line comes in; the yellow line indicates the cumulative proportion of variance explained if you included all principal components up to that point. For example, the yellow dot above PC2 indicates that including principal components 1 and 2 will explain about 42% of the total variance in the model.Now let\xe2\x80\x99s go through some examples:Method 1: We arbitrarily select a number of principal components to include. Suppose I wanted to keep five principal components in my model. In the genetic data case above, these five principal components explains about 66% of the total variability that would be explained by including all 13 principal components.Method 2: Suppose I wanted to include enough principal components to explain 90% of the total variability explained by all 13 principal components. In the genetic data case above, I would include the first 10 principal components and drop the final three variables from Z*.Method 3: Here, we want to \xe2\x80\x9cfind the elbow.\xe2\x80\x9d In the scree plot above, we see there\xe2\x80\x99s a big drop in proportion of variability explained between principal component 2 and principal component 3. In this case, we\xe2\x80\x99d likely include the first two features and drop the remaining features. As you can see, this method is a bit subjective as \xe2\x80\x9celbow\xe2\x80\x9d doesn\xe2\x80\x99t have a mathematically precise definition and, in this case, we\xe2\x80\x99d include a model that explains only about 42% of the total variability.(Note: Some scree plots will have the size of eigenvectors on the Y axis rather than the proportion of variance. This leads to equivalent results, but requires the user to manually calculate the proportion of variance. An example of this can be seen here.)Once we\xe2\x80\x99ve dropped the transformed variables we want to drop, we\xe2\x80\x99re done! That\xe2\x80\x99s PCA.But, like, *why* does PCA\xc2\xa0work?While PCA is a very technical method relying on in-depth linear algebra algorithms, it\xe2\x80\x99s a relatively intuitive method when you think about it.First, the covariance matrix Z\xe1\xb5\x80Z is a matrix that contains estimates of how every variable in Z relates to every other variable in Z. Understanding how one variable is associated with another is quite powerful.Second, eigenvalues and eigenvectors are important. Eigenvectors represent directions. Think of plotting your data on a multidimensional scatterplot. Then one can think of an individual eigenvector as a particular \xe2\x80\x9cdirection\xe2\x80\x9d in your scatterplot of data. Eigenvalues represent magnitude, or importance. Bigger eigenvalues correlate with more important directions.Finally, we make an assumption that more variability in a particular direction correlates with explaining the behavior of the dependent variable. Lots of variability usually indicates signal, whereas little variability usually indicates noise. Thus, the more variability there is in a particular direction is, theoretically, indicative of something important we want to detect. (The setosa.io PCA applet is a great way to play around with data and convince yourself why it makes sense.)Thus, PCA is a method that brings together:A measure of how each variable is associated with one another. (Covariance matrix.)The directions in which our data are dispersed. (Eigenvectors.)The relative importance of these different directions. (Eigenvalues.)PCA combines our predictors and allows us to drop the eigenvectors that are relatively unimportant.Are there extensions to\xc2\xa0PCA?Yes, more than I can address here in a reasonable amount of space. The one I\xe2\x80\x99ve most frequently seen is principal component regression, where we take our untransformed Y and regress it on the subset of Z* that we didn\xe2\x80\x99t drop. (This is where the independence of the columns of Z* comes in; by regressing Y on Z*, we know that the required independence of independent variables will necessarily be satisfied. However, we will need to still check our other assumptions.)The other commonly-seen variant I\xe2\x80\x99ve seen is kernel PCA.ConclusionI hope you found this article helpful! Check out some of the resources below for more in-depth discussions of PCA. Let me know what you think, especially if there are suggestions for improvement.I\xe2\x80\x99ve been told that a Chinese translation of this article has been made available here. (Thanks, Jakukyo Friel!)I want to offer many thanks to my friends Ritika Bhasker, Joseph Nelson, and Corey Smith for their suggestions and edits. You should check Ritika and Joseph out on Medium\xe2\x80\x8a\xe2\x80\x94\xe2\x80\x8atheir posts are far more entertaining than mine. (Corey is too focused on not getting his Ph.D. research scooped to have a Medium presence.)I also want to give a huge h/t to the setosa.io applet for its visual and intuitive display of PCA.Edit: Thanks to Michael Matthews for noticing a typo in the formula for Z* in Step 7 above. He correctly pointed out that Z* = ZP*, not Z\xe1\xb5\x80P*. Thanks also to Chienlung Cheung for noticing another typo in Step 8 above and noted that I had conflated \xe2\x80\x9ceigenvector\xe2\x80\x9d with \xe2\x80\x9ceigenvalue\xe2\x80\x9d in one line.Resources You Should Check\xc2\xa0Out:This is a list of resources I used to compile this PCA article as well as other resources I\xe2\x80\x99ve generally found helpful to understand PCA. If you know of any resources that would be a good inclusion to this list, please leave a comment and I\xe2\x80\x99ll add them.Non-Academic Articles and ResourcesSetosa.io\xe2\x80\x99s PCA applet. (An applet that allows you to visualize what principal components are and how your data affect the principal components.)A semi-academic walkthrough of building blocks to the PCA algorithm and the algorithm itself.The top answer to this StackExchange question is, in a word, outstanding.A CrossValidated question and answer discussing whether there are parametric assumptions to PCA. (Spoiler alert: PCA itself is a nonparametric method, but regression or hypothesis testing after using PCA might require parametric assumptions.)A resource list would hardly be complete without the Wikipedia link, right? (Despite Wikipedia being low-hanging fruit, it has an solid list of additional links and resources at the bottom of the page.)Coding ResourcesPython Documentation for PCA within the sklearn library. (This link includes examples!)PCA Explanation on AnalyticsVidhya. (This link includes Python and R.)Implementing PCA in Python with a few cool plots.Comparison of methods for implementing PCA in R.Academic Textbooks and\xc2\xa0ArticlesAn Introduction to Statistical Learning, 6th printing, by James, Witten, Hastie, and Tibshirani. (PCA is covered extensively in chapters 6.3, 6.7, and 10.2. This book assumes knowledge of linear regression but is pretty accessible, all things considered.)Notes from Penn State\xe2\x80\x99s STAT 505 (Applied Multivariate Statistical Analysis) Course. (I\xe2\x80\x99ve found Penn State\xe2\x80\x99s online statistics course notes to be incredible, and the PCA section here is particularly helpful.)Linear Algebra and Its Applications, 4th edition, by David Lay. (PCA is covered in chapter 7.5.)A Tutorial on Principal Components Analysis, by Jonathon Shlens at Google Research.A draft chapter on Principal Component Analysis from Cosma Shalizi of Carnegie Mellon University.A chapter on data preprocessing from Applied Predictive Modeling includes an introductory discussion of principal component analysis (with visuals!) in Section 3.3. (h/t to Jay Lucas for the recommendation!)Elements of Statistical Learning, 10th printing, by Hastie, Tibshirani, and Friedman. (PCA is covered extensively in chapters 3.5, 14.5, and 18.6. This book assumes knowledge of linear regression, matrix algebra, and calculus and is significantly more technical than An Introduction to Statistical Learning, but the two follow a similar structure given the common authors.)Tangential ResourcesEssence of Linear Algebra YouTube Series (Including one video on Eigenvectors and Eigenvalues that is especially relevant to PCA; h/t to Tim Book for making me aware of this incredible resource.)Data ScienceMachine LearningStatisticsProgrammingTowards Data Science6K claps36BlockedUnblockFollowFollowingMatt BremsData Scientist | Educator | Political EnthusiastFollowTowards Data ScienceSharing concepts, ideas, and codes.6KNever miss a story from Towards Data Science, when you sign up for Medium. Learn moreGet updatesGet updates'b'\nBack\n\n\nPrincipal Component Analysis\nExplained Visually\n\nTweet\n\n\n\nBy Victor Powell\n\nwith text by Lewis Lehe\n\nPrincipal component analysis (PCA) is a technique used to emphasize variation and bring out strong patterns in a dataset.  It\'s often used to make data easy to explore and visualize.\n2D example\nFirst, consider a dataset in only two dimensions, like (height, weight). This dataset can be plotted as points in a plane. But if we want to tease out variation, PCA finds a new coordinate system in which every point has a new (x,y) value. The axes don\'t actually mean anything physical; they\'re combinations of height and weight called "principal components" that are chosen to give one axes lots of variation.\nDrag the points around in the following visualization to see PC coordinate system adjusts.\n\n\n\n\nPCA is useful for eliminating dimensions. Below, we\'ve plotted the data along a pair of lines: one composed of the x-values and another of the y-values.\n\n\nIf we\'re going to only see the data along one dimension, though, it might be better to make that dimension the principal component with most variation. We don\'t lose much by dropping PC2 since it contributes the least to the variation in the data set.\n\n\n\n\n\n3D example\nWith three dimensions, PCA is more useful, because it\'s hard to see through a cloud of data. In the example below, the original data are plotted in 3D, but you can project the data into 2D through a transformation no different than finding a camera angle: rotate the axes to find the best angle. To see the "official" PCA transformation, click the "Show PCA" button. The PCA transformation ensures that the horizontal axis PC1 has the most variation, the vertical axis PC2 the second-most, and a third axis PC3 the least. Obviously, PC3 is the one we drop.\n\n\nshow PCA\nreset\n\n\n\nEating in the UK (a 17D example)\nOriginal example from Mark Richardson\'s class notes Principal Component Analysis\nWhat if our data have way more than 3-dimensions? Like, 17 dimensions?! In the table is the average consumption of 17 types of food in grams per person per week for every country in the UK. \nThe table shows some interesting variations across different food types, but overall differences aren\'t so notable. Let\'s see if PCA can eliminate dimensions to emphasize how countries differ.\n\n\n\n\n\nHere\'s the plot of the data along the first principal component. Already we can see something is different about Northern Ireland.\n\n\n\nNow, see the first and second principal components, we see Northern Ireland a major outlier. Once we go back and look at the data in the table, this makes sense: the Northern Irish eat way more grams of fresh potatoes and way fewer of fresh fruits, cheese, fish and alcoholic drinks. It\'s a good sign that structure we\'ve visualized reflects a big fact of real-world geography: Northern Ireland is the only of the four countries not on the island of Great Britain. (If you\'re confused about the differences among England, the UK and Great Britain, see: this video.)\n\n\n\n\n\nFor more explanations, visit the Explained Visually project homepage.\nOr subscribe to our mailing list.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to view the comments powered by Disqus.\ncomments powered by Disqus\n\n'b'\nBack\n\n\nPrincipal Component Analysis\nExplained Visually\n\nTweet\n\n\n\nBy Victor Powell\n\nwith text by Lewis Lehe\n\nPrincipal component analysis (PCA) is a technique used to emphasize variation and bring out strong patterns in a dataset.  It\'s often used to make data easy to explore and visualize.\n2D example\nFirst, consider a dataset in only two dimensions, like (height, weight). This dataset can be plotted as points in a plane. But if we want to tease out variation, PCA finds a new coordinate system in which every point has a new (x,y) value. The axes don\'t actually mean anything physical; they\'re combinations of height and weight called "principal components" that are chosen to give one axes lots of variation.\nDrag the points around in the following visualization to see PC coordinate system adjusts.\n\n\n\n\nPCA is useful for eliminating dimensions. Below, we\'ve plotted the data along a pair of lines: one composed of the x-values and another of the y-values.\n\n\nIf we\'re going to only see the data along one dimension, though, it might be better to make that dimension the principal component with most variation. We don\'t lose much by dropping PC2 since it contributes the least to the variation in the data set.\n\n\n\n\n\n3D example\nWith three dimensions, PCA is more useful, because it\'s hard to see through a cloud of data. In the example below, the original data are plotted in 3D, but you can project the data into 2D through a transformation no different than finding a camera angle: rotate the axes to find the best angle. To see the "official" PCA transformation, click the "Show PCA" button. The PCA transformation ensures that the horizontal axis PC1 has the most variation, the vertical axis PC2 the second-most, and a third axis PC3 the least. Obviously, PC3 is the one we drop.\n\n\nshow PCA\nreset\n\n\n\nEating in the UK (a 17D example)\nOriginal example from Mark Richardson\'s class notes Principal Component Analysis\nWhat if our data have way more than 3-dimensions? Like, 17 dimensions?! In the table is the average consumption of 17 types of food in grams per person per week for every country in the UK. \nThe table shows some interesting variations across different food types, but overall differences aren\'t so notable. Let\'s see if PCA can eliminate dimensions to emphasize how countries differ.\n\n\n\n\n\nHere\'s the plot of the data along the first principal component. Already we can see something is different about Northern Ireland.\n\n\n\nNow, see the first and second principal components, we see Northern Ireland a major outlier. Once we go back and look at the data in the table, this makes sense: the Northern Irish eat way more grams of fresh potatoes and way fewer of fresh fruits, cheese, fish and alcoholic drinks. It\'s a good sign that structure we\'ve visualized reflects a big fact of real-world geography: Northern Ireland is the only of the four countries not on the island of Great Britain. (If you\'re confused about the differences among England, the UK and Great Britain, see: this video.)\n\n\n\n\n\nFor more explanations, visit the Explained Visually project homepage.\nOr subscribe to our mailing list.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to view the comments powered by Disqus.\ncomments powered by Disqus\n\n'b'\n\n\n\n\nPlease turn JavaScript on and reload the page.\n\n\n\n\n\n\nChecking your browser before accessing statisticssolutions.com.\nThis process is automatic. Your browser will redirect to your requested content shortly.\nPlease allow up to 5 seconds\xe2\x80\xa6\n\n\n\n\n\n\n\n\n\nDDoS protection by Cloudflare\n\n            Ray ID: 4cc6abb1e917c338\n          \n\n\n\n'